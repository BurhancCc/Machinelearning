import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense,LSTM,Dropout, Flatten
from keras.preprocessing import sequence, timeseries
import keras_tuner
from keras import backend as K, losses

#### Load data
df = pd.read_csv('MachineLearning\editedFIFA.csv')

# used to determine the bins to classifier
mean = df['Release Clause'].mean()
std = df['Release Clause'].std()

# split into train test sets
X = df[['Age', 'Overall', 'Potential', 'Value', 'Wage', 'Special', 'International Reputation', 'Weak Foot', 'Skill Moves', 'Height', 'Weight', 'Crossing', 'Finishing',
        'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',	'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower', 'Jumping', 'Stamina', 'Strength',
        'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes']]
y = df['Release Clause']

# creating our train, and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 5)


model_seq = Sequential()
model_seq.add(Dense(5, activation='sigmoid', input_shape=(X_train.shape[1],)))
model_seq.add(Dense(4, activation='sigmoid'))
model_seq.add(Dense(1, activation='softmax'))
model_seq.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
model_seq.fit(X_train, y_train, epochs=20, validation_data=(X_test,y_test))

def model_builder(hp):
  '''
  Args:
    hp - Keras tuner object
  '''
  # Initialize the Sequential API and start stacking the layers
  model = Sequential()
  model.add(Flatten(input_shape=(28, 28)))
  # Tune the number of units in the first Dense layer
  # Choose an optimal value between 32-512
  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)
  model.add(Dense(units=hp_units, activation='relu', name='dense_1'))
  # Add next layers
  model.add(Dropout(0.2))
  model.add(Dense(10, activation='softmax'))
  # Tune the learning rate for the optimizer
  # Choose an optimal value from 0.01, 0.001, or 0.0001
  hp_learning_rate = hp.Float("learning_rate", min_value=1e-4, max_value=1e-2, sampling="log")
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=hp_learning_rate),
                loss=losses.SparseCategoricalCrossentropy(),
                metrics=['accuracy'])
  return model

# Instantiate the tuner
tuner = keras_tuner.Hyperband(model_builder, objective='val_accuracy', max_epochs=20, factor=3,
directory='HHNK_DS_PROJECT',  project_name='HHNK Data Science Project')

# hypertuning settings
tuner.search_space_summary()

#stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)
# Perform hypertuning
#tuner.search(x_train, y_train, epochs=10, validation_split=0.2, callbacks=[stop_early])
tuner.search(x, y, epochs=20, validation_split=0.2)
best_model = tuner.get_best_hyperparameters()[0]
print("BEST MODEL")
print(best_model)
